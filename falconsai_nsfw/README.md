# ğŸš€ NSFW Image Detection API (Flask + Hugging Face ViT)

This project provides a **production-ready, Dockerized Flask API** for NSFW image classification using [FalconsAIâ€™s ViT-based model](https://huggingface.co/Falconsai/nsfw_image_detection).

It supports **both CPU and GPU deployments** via Docker Compose.

---

## ğŸ“‚ Project Structure

```
nsfw-api-flask/
â”œâ”€ app/
â”‚  â”œâ”€ __init__.py         # Flask app factory + health endpoints
â”‚  â”œâ”€ app.py              # API routes (/predict)
â”‚  â”œâ”€ model.py            # Model loading + inference
â”‚  â”œâ”€ settings.py         # Configuration via env vars
â”‚  â””â”€ logging_conf.py     # JSON logging setup
â”œâ”€ Dockerfile             # CPU image
â”œâ”€ Dockerfile.gpu         # GPU image (CUDA runtime)
â”œâ”€ docker-compose.yml     # Orchestration (CPU + GPU services)
â”œâ”€ gunicorn.conf.py       # Production server config
â”œâ”€ requirements.txt       # Python dependencies
â””â”€ README.md              # ğŸ“– You are here
```

---

## âœ¨ Features

- âœ… **Flask REST API** with `/predict`, `/healthz`, `/ready` endpoints
- âœ… **CPU or GPU inference** (switch via env var `USE_GPU`)
- âœ… **JSON logging** (easy integration with ELK / Loki / CloudWatch)
- âœ… **Upload size limits** (`MAX_CONTENT_LENGTH_MB`)
- âœ… **Graceful error handling** (bad images, missing files, unsupported formats)
- âœ… **Dockerized for production** with Gunicorn server
- âœ… **Docker Compose setup** for running both CPU and GPU services

---

## âš™ï¸ Requirements

- Docker â‰¥ 20.x
- (Optional for GPU) NVIDIA drivers + [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)

---

## ğŸ”§ Configuration

All runtime options are controlled via **environment variables**:

| Variable                | Default                          | Description                          |
| ----------------------- | -------------------------------- | ------------------------------------ |
| `MODEL_NAME`            | `Falconsai/nsfw_image_detection` | Hugging Face model to load           |
| `USE_GPU`               | `false`                          | Set `true` to run on GPU             |
| `MAX_CONTENT_LENGTH_MB` | `10`                             | Max upload size (in MB)              |
| `LOG_LEVEL`             | `INFO`                           | Logging verbosity                    |
| `LOG_JSON`              | `true`                           | JSON logs (`false` = human-readable) |

---

## ğŸ³ Running with Docker Compose

### 1. Build & start services

```bash
docker compose up --build -d
```

This launches:

- **CPU service** â†’ `http://localhost:5000`
- **GPU service** â†’ `http://localhost:5001`

_(GPU container only works if host has NVIDIA GPU drivers + toolkit installed.)_

### 2. Check logs

```bash
docker compose logs -f nsfwpy-cpu
```

---

## ğŸŒ API Endpoints

### Health & Readiness

```bash
curl http://localhost:5000/health
# {"status": "ok"}

curl http://localhost:5000/ready
# {"ready": true}
```

### Prediction

```bash
curl -X POST http://localhost:5000/classify/binary \
  -F "file=@/path/to/image.jpg"
```

**Response:**

```json
{
  "classification": "nsfw",
  "probability": 0.9842,
  "scores": {
    "nsfw": 0.9842,
    "normal": 0.0158
  }
}
```

---

## ğŸ–¥ï¸ Development (without Docker)

1. Create a virtual environment & install dependencies:

   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```

2. Run locally:

   ```bash
   gunicorn -c gunicorn.conf.py app:create_app()
   ```

   API will be available at: `http://localhost:8000`

---

## ğŸ”’ CORS (Do You Need It?)

- If this API is **used only internally** (e.g., behind a backend or gateway), **no CORS needed**.
- If it will be **called directly from browsers** (JS apps, frontend clients), enable [Flask-CORS](https://flask-cors.readthedocs.io/en/latest/).

Example (add in `app/__init__.py`):

```python
from flask_cors import CORS
CORS(app, resources={r"/predict": {"origins": "*"}})
```

---

## ğŸš€ Deployment Tips

- Scale containers with Docker Compose or Kubernetes (one process per GPU recommended).
- Place behind **NGINX / API Gateway / Load Balancer**.
- Use **API keys / auth middleware** if exposed publicly.
- Monitor logs for latency and errors.

---

## ğŸ“Œ Example: Compose Override for GPU Only

If you only want GPU service, run:

```bash
docker compose up --build nsfwpy-gpu
```

Or CPU only:

```bash
docker compose up --build nsfwpy-cpu
```

---

## ğŸ“ License

This repo is for research/educational use. The model (`Falconsai/nsfw_image_detection`) is released on Hugging Face under its respective license.
